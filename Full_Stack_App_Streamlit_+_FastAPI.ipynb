{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**<h1>A. Application </h1>**"
      ],
      "metadata": {
        "id": "UyNT0RpAJrU8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cr√©er une **application chatbot avec gestion de sessions** :\n",
        "- **Frontend** : Interface utilisateur avec Streamlit.\n",
        "- **Backend** : R√©pond √† l'utilisateur via une API FastAPI.\n",
        "\n",
        "---\n",
        "\n",
        "**<h2> üõ†Ô∏è 1. Installation des biblioth√®ques </h2>**\n",
        "\n",
        "Installer les biblioth√®ques n√©cessaires dans un environnement virtuel (ou globalement) avec pip :\n",
        "\n",
        "```bash\n",
        "pip install streamlit fastapi uvicorn requests\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "**<h2> üìÅ 2. Cr√©ation du fichier `frontend.py`</h2>**\n",
        "\n",
        "Ce fichier contient **l‚Äôinterface utilisateur** via Streamlit.\n",
        "\n",
        "> üìÑ Cr√©ez un fichier appel√© `frontend.py` et collez le code ci-dessous :\n",
        "\n",
        "```python\n",
        "import streamlit as st\n",
        "import requests\n",
        "import time\n",
        "from requests.utils import quote\n",
        "\n",
        "# Fonction qui envoie le prompt √† l'API backend et r√©cup√®re la r√©ponse\n",
        "def response_generator(prompt):\n",
        "    encoded_prompt = quote(prompt, safe=\"\")  # Encode le prompt pour √©viter les erreurs d'URL\n",
        "    response = requests.get(f\"http://localhost:8000/llm/{encoded_prompt}\")\n",
        "    response_text = response.json().get(\"answer\", \"Erreur : aucune r√©ponse re√ßue.\")\n",
        "\n",
        "    # G√©n√®re la r√©ponse mot par mot avec un petit d√©lai pour effet \"chat\"\n",
        "    for word in response_text.split():\n",
        "        yield word + \" \"\n",
        "        time.sleep(0.05)\n",
        "\n",
        "# Titre principal\n",
        "st.title(\"Chatbot avec gestion des sessions\")\n",
        "\n",
        "# Initialisation de la variable de session si elle n'existe pas\n",
        "if \"sessions\" not in st.session_state:\n",
        "    st.session_state.sessions = {\"Session 1\": []}  # Une session par d√©faut\n",
        "if \"current_session\" not in st.session_state:\n",
        "    st.session_state.current_session = \"Session 1\"\n",
        "\n",
        "# Barre lat√©rale pour choisir ou cr√©er des sessions\n",
        "st.sidebar.title(\"Sessions de Chat\")\n",
        "session_names = list(st.session_state.sessions.keys())\n",
        "selected_session = st.sidebar.selectbox(\"S√©lectionnez une session\", session_names)\n",
        "st.session_state.current_session = selected_session\n",
        "\n",
        "# Affiche l'historique de la session s√©lectionn√©e\n",
        "st.subheader(f\"Chat - {st.session_state.current_session}\")\n",
        "for message in st.session_state.sessions[selected_session]:\n",
        "    with st.chat_message(message[\"role\"]):\n",
        "        st.markdown(message[\"content\"])\n",
        "\n",
        "# Cr√©er une nouvelle session\n",
        "if st.sidebar.button(\"Cr√©er une nouvelle session\"):\n",
        "    new_session_name = f\"Session {len(st.session_state.sessions) + 1}\"\n",
        "    st.session_state.sessions[new_session_name] = []\n",
        "    st.session_state.current_session = new_session_name\n",
        "    st.rerun()  # Recharge la page avec la nouvelle session\n",
        "\n",
        "# Zone d'entr√©e utilisateur\n",
        "if prompt := st.chat_input(\"Que voulez-vous dire ?\"):\n",
        "    # Ajoute le message utilisateur\n",
        "    st.session_state.sessions[st.session_state.current_session].append(\n",
        "        {\"role\": \"user\", \"content\": prompt}\n",
        "    )\n",
        "    with st.chat_message(\"user\"):\n",
        "        st.markdown(prompt)\n",
        "\n",
        "    # Appelle le backend et affiche la r√©ponse\n",
        "    with st.chat_message(\"assistant\"):\n",
        "        response = st.write_stream(response_generator(prompt))\n",
        "\n",
        "    # Sauvegarde la r√©ponse dans la session\n",
        "    st.session_state.sessions[st.session_state.current_session].append(\n",
        "        {\"role\": \"assistant\", \"content\": response}\n",
        "    )\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "**<h2> ‚öôÔ∏è 3. Cr√©ation du fichier `backend.py`</h2>**\n",
        "\n",
        "> üìÑ Cr√©ez un fichier `backend.py` avec ce code :\n",
        "\n",
        "```python\n",
        "from fastapi import FastAPI\n",
        "import uvicorn\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "# Route qui prend un prompt et retourne une r√©ponse simul√©e\n",
        "@app.get(\"/llm/{prompt}\")\n",
        "async def get_response(prompt: str):\n",
        "    return {\"answer\": f\"Hey, My name is Chatbot and you say: {prompt}\"}\n",
        "\n",
        "# Lancer l'API si on ex√©cute le fichier directement\n",
        "if __name__ == \"__main__\":\n",
        "    uvicorn.run(app, host=\"localhost\", port=8000)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "**<h2> üöÄ 4. Ex√©cution des fichiers </h2>**\n",
        "\n",
        "Maintenant que tout est pr√™t, on va lancer les deux parties de l'application :\n",
        "\n",
        "**<h3> √âtape 1 : Lancer le backend FastAPI</h3>**\n",
        "\n",
        "Dans un terminal, ex√©cute :\n",
        "\n",
        "```bash\n",
        "python backend.py\n",
        "```\n",
        "\n",
        "Cela lance un serveur API sur `http://localhost:8000`.\n",
        "\n",
        "**<h3> √âtape 2 : Lancer le frontend Streamlit</h3>**\n",
        "\n",
        "Dans un autre terminal :\n",
        "\n",
        "```bash\n",
        "streamlit run frontend.py\n",
        "```\n",
        "\n",
        "Cela ouvre automatiquement une interface web (g√©n√©ralement √† `http://localhost:8501`).\n",
        "\n",
        "---\n",
        "\n",
        "**<h2> ‚úÖ R√©sultat attendu </h2>**\n",
        "\n",
        "- Une interface web simple.\n",
        "- Gestion de plusieurs **sessions de chat**.\n",
        "- Le chatbot r√©pond dynamiquement √† chaque message envoy√©.\n",
        "- Backend simule une r√©ponse mais peut √™tre remplac√© plus tard par un vrai mod√®le IA.\n",
        "\n",
        "---\n",
        "\n",
        "**<h2> üßë‚Äçüè´ Pistes d‚Äôapprofondissement </h2>**\n",
        "\n",
        "- Int√©grer **HuggingFace** ou **LangChain** dans le backend.\n",
        "- Sauvegarder les conversations avec une base de donn√©es.\n",
        "- D√©ployer le frontend avec **[Streamlit Cloud](https://www.youtube.com/watch?v=JL9xOs-G1hI)** et le backend sur **[Render](https://www.youtube.com/watch?v=stDadw38H0I)** ou **Railway**."
      ],
      "metadata": {
        "id": "EO8QrtbQ0wts"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<h1>B. Ex√©cution des LLMs</h1>**"
      ],
      "metadata": {
        "id": "P45m8m5MJJhD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<h2>1. üåê Utiliser un LLM via Hugging Face Inference Client</h2>**\n",
        "\n",
        "**Avantage** : Simple, rapide, h√©berg√© (pas besoin de GPU local).\n",
        "\n",
        " ‚úÖ √âtapes :\n",
        "1. Installe la biblioth√®que :\n",
        "```bash\n",
        "pip install huggingface_hub\n",
        "```\n",
        "\n",
        "2. Code :\n",
        "\n",
        "```python\n",
        "from huggingface_hub import InferenceClient\n",
        "import os\n",
        "\n",
        "os.environ[\"HF_TOKEN\"] = \"TA_CL√â_HF\"\n",
        "\n",
        "client = InferenceClient(\"mistralai/Mistral-7B-Instruct-v0.3\", token=os.getenv(\"HF_TOKEN\"))\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": \"What is the meaning of life?\"}\n",
        "]\n",
        "\n",
        "response = client.chat_completion(messages, max_tokens=1024)\n",
        "print(response[\"choices\"][0][\"message\"][\"content\"])\n",
        "```\n",
        "\n",
        "üìå *Utilise les mod√®les HF comme une API s√©curis√©e.*\n",
        "\n",
        "---\n",
        "\n",
        "**<h2> 2. üíª Utiliser un LLM en local avec Llama.cpp</h2>**\n",
        "\n",
        "**Avantage** : 100% local, pas besoin d'acc√®s internet ou d‚ÄôAPI.\n",
        "\n",
        "‚úÖ √âtapes :\n",
        "\n",
        "1. Installer la lib et t√©l√©charger le mod√®le :\n",
        "```bash\n",
        "pip install llama-cpp-python\n",
        "wget https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q2_K.gguf -O mistral.gguf\n",
        "```\n",
        "\n",
        "2. Code :\n",
        "\n",
        "```python\n",
        "from llama_cpp import Llama\n",
        "\n",
        "llm = Llama(model_path=\"mistral.gguf\")\n",
        "\n",
        "output = llm(\n",
        "   \"Q: Who is the president of USA? A: \",\n",
        "   max_tokens=1024,\n",
        "   stop=[\"Q:\", \"\\n\"],\n",
        "   echo=True\n",
        ")\n",
        "\n",
        "print(output[\"choices\"][0][\"text\"])\n",
        "```\n",
        "\n",
        "üìå *Id√©al pour les apps sans connexion ou avec contraintes de confidentialit√©.*\n",
        "\n",
        "---\n",
        "\n",
        "**<h2> 3. ‚òÅÔ∏è Utiliser un LLM via DeepInfra (API Huggingface-compatible)</h2>**\n",
        "\n",
        "**Avantage** : Interface de type OpenAI, simple, rapide √† tester.\n",
        "\n",
        "‚úÖ √âtapes :\n",
        "1. Cr√©e un compte sur [deepinfra.com](https://deepinfra.com) et r√©cup√®re ton `api_key`.\n",
        "\n",
        "2. Code :\n",
        "\n",
        "```python\n",
        "import requests\n",
        "\n",
        "api_key = \"TA_CL√â_DEEPINFRA\"\n",
        "url = \"https://api.deepinfra.com/v1/openai/chat/completions\"\n",
        "\n",
        "headers = {\n",
        "    \"Authorization\": f\"Bearer {api_key}\",\n",
        "    \"Content-Type\": \"application/json\"\n",
        "}\n",
        "\n",
        "data = {\n",
        "    \"model\": \"mistralai/Mistral-7B-Instruct-v0.1\",\n",
        "    \"messages\": [\n",
        "        {\"role\": \"system\", \"content\": \"Tu es un assistant utile.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Comment pr√©venir le diab√®te ?\"}\n",
        "    ],\n",
        "    \"temperature\": 0.7,\n",
        "    \"max_tokens\": 1024\n",
        "}\n",
        "\n",
        "res = requests.post(url, headers=headers, json=data)\n",
        "print(res.json()[\"choices\"][0][\"message\"][\"content\"])\n",
        "```\n",
        "\n",
        "üìå *Parfait pour prototyper des assistants dans des apps web ou mobiles.*\n",
        "\n",
        "---\n",
        "\n",
        "**<h2>üß© Bonus : Autres APIs possibles</h2>**\n",
        "\n",
        "- **OpenAI API** (`gpt-3.5`, `gpt-4`) : tr√®s puissante, payante.\n",
        "- **[Replicate](https://replicate.com/)**, **[Together.ai](Together.ai)**, **[Anyscale](https://www.anyscale.com/blog/anyscale-endpoints-fast-and-scalable-llm-apis)** : offrent aussi des mod√®les open-source via API.\n",
        "- **LangChain** : pour cha√Æner des appels de LLM avec logique m√©tier.\n",
        "\n",
        "---\n",
        "\n",
        "**<h2>üéì En r√©sum√©</h2>**\n",
        "\n",
        "| M√©thode        | Niveau technique | Avantages                         | Inconv√©nients                    |\n",
        "|----------------|------------------|-----------------------------------|----------------------------------|\n",
        "| Hugging Face   | Facile           | H√©berg√©, rapide, fiable           | Besoin d‚Äôun token                |\n",
        "| Llama.cpp      | Moyen            | Local, gratuit, confidentiel      | Mod√®le lourd √† t√©l√©charger       |\n",
        "| DeepInfra      | Facile           | Compatible OpenAI, simple √† coder | API key n√©cessaire               |\n",
        "\n"
      ],
      "metadata": {
        "id": "EhBbOKJvMAEc"
      }
    }
  ]
}